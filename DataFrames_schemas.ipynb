{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "\n",
    "<https://www.kaggle.com/c/tmdb-box-office-prediction/data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySparkShell\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_rdd = sc.parallelize([\n",
    "    (\"MAD\", \"Madrid\", \"ES\", 40.4165, -3.70256),\n",
    "    (\"BCN\", \"Barcelona\", \"ES\", 41.297078, 2.078464),\n",
    "    (\"PAR\", \"Paris\", \"FR\", 48.85341, 2.3488),\n",
    "    (\"ROM\", \"Rome\", \"IT\", 41.89193, 12.51133)])\n",
    "\n",
    "cities_df = cities_rdd.toDF([\n",
    "    \"city_code\",\"city_name\",\"country_code\",\"latitude\",\"longitude\"])\n",
    "\n",
    "cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = spark.createDataFrame([\n",
    "    (\"MAD\", \"Madrid\", \"ES\", 40.4165, -3.70256),\n",
    "    (\"BCN\", \"Barcelona\", \"ES\", 41.297078, 2.078464),\n",
    "    (\"PAR\", \"Paris\", \"FR\", 48.85341, 2.3488),\n",
    "    (\"ROM\", \"Rome\", \"IT\", 41.89193, 12.51133)],\n",
    "    [\"city_code\",\"city_name\",\"country_code\",\"latitude\",\"longitude\"])\n",
    "\n",
    "cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df.filter(cities_df.country_code==\"ES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df.select('city_code','country_code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F \n",
    "agg_df = cities_df\\\n",
    "    .groupBy('country_code')\\\n",
    "    .agg(F.count(F.col('city_code')))\\\n",
    "    .orderBy(F.col('country_code'))\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agg_df.rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array and Struct types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = sc.parallelize([\n",
    "    (\"ES\", [\"Spanish\",\"Catalan\",\"Basque\"], (\"MAD\", \"Madrid\")),\n",
    "    (\"FR\", [\"French\",\"Alsacien\",\"Breton\"], (\"PAR\", \"Paris\")),\n",
    "    (\"IT\", [\"Italian\",\"French\"], (\"ROM\", \"Rome\")),\n",
    "    (\"US\", [\"English\", \"Spanish\"], (\"WAS\", \"Washington\"))]).toDF([\"country_code\", \"languages\", \"capital\"])\\\n",
    "    .withColumn('capital', F.struct(F.col(\"capital._1\").alias(\"code\"), F.col(\"capital._2\").alias(\"name\")))\n",
    "countries.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.select('country_code',F.col('languages').getItem(0),'capital.name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/JulienCojan/pyspark_kschool/master/data/tmdb-box-office-prediction/train.csv -P data/tmdb-box-office-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf = spark\\\n",
    "    .read\\\n",
    "    .csv(\"data/tmdb-box-office-prediction/train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(films_sdf.select(\"cast\").limit(10).toPandas().iloc[1][\"cast\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "films_sdf = films_sdf\\\n",
    "    .withColumn(\"id\", films_sdf.id.cast(IntegerType()))\n",
    "    \n",
    "    \n",
    "films_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.select(\"genres\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"id\", IntegerType()),\n",
    "        StructField(\"name\", StringType())\n",
    "    ])\n",
    ")\n",
    "\n",
    "films_sdf = films_sdf\\\n",
    "    .withColumn(\"genres\", F.from_json(F.col(\"genres\"), schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.select(\"genres\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.select(\"genres.name\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.select(\"genres.name\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf2 = spark.read.csv(\"data/tmdb-box-office-prediction/train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf2.select(\"genres\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_col = films_sdf2\\\n",
    "    .select(\n",
    "    \"id\", \n",
    "    F.col(\"genres\").substr(F.lit(2), F.length(F.col(\"genres\"))-2).alias(\"genres\"))\n",
    "\n",
    "genres_col.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_col\\\n",
    "     .withColumn(\"genres_array\", F.split(F.col(\"genres\"), '(?<=}), '))\\\n",
    "     .head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sdf = genres_col\\\n",
    "    .select(\"id\", F.explode(F.split(F.col(\"genres\"), '(?<=}), ')).alias(\"genre\"))\n",
    "\n",
    "genre_sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sdf.select(\n",
    "    F.col('id'),\n",
    "    F.regexp_extract(F.col(\"genre\"), \"(?<='name':\\ ')[A-Z][a-z]+\", 0).alias(\"genre_name\")\n",
    "    ).groupBy(\"genre_name\")\\\n",
    "    .count().orderBy(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf.repartition(2).write.mode('overwrite').parquet(\"data/tmdb1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/tmdb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_films_sdf = spark.read.parquet(\"data/tmdb1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_films_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, LongType\n",
    "\n",
    "schema = \\\n",
    "StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"genres\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", IntegerType()),\n",
    "            StructField(\"name\", StringType())\n",
    "        ])))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_films_sdf = spark.read.schema(schema=schema).parquet(\"data/tmdb1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_films_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/JulienCojan/pyspark_kschool/master/data/competitive-data-science-predict-future-sales/sales_train.csv.gz -P data/competitive-data-science-predict-future-sales/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sdf = spark.read.option(\"header\", \"true\").csv(\"data/competitive-data-science-predict-future-sales/sales_train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "sales_with_iso_dates = sales_sdf\\\n",
    "    .withColumn(\"date\", F.from_unixtime(F.unix_timestamp(F.col(\"date\"), 'dd.MM.yyyy')))\\\n",
    "    .withColumn(\"year\", F.year(\"date\"))\\\n",
    "    .withColumn(\"month\", F.month(\"date\"))\\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_iso_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_iso_dates.write.partitionBy(\"year\", \"month\", \"day\").mode('overwrite').parquet(\"data/tmdb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/tmdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/tmdb2/year=2013/month=3/day=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf2 = spark.read.parquet(\"data/tmdb2/year=2013\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf3 = spark\\\n",
    "    .read\\\n",
    "    .option(\"basePath\", \"data/tmdb2\")\\\n",
    "    .parquet(\n",
    "        \"data/tmdb2/year=2014/month=3\",\n",
    "        \"data/tmdb2/year=2014/month=4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sdf3.groupBy(\"year\", \"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    Row(id=1, value=15.0),\n",
    "    Row(id=2, value=None),\n",
    "    Row(id=3, value=float('NaN')),\n",
    "])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([\n",
    "    Row(id=1, value=float('NaN')),\n",
    "    Row(id=2, value=42.0),\n",
    "    Row(id=3, value=None)\n",
    "])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, df1[\"value\"] == df2[\"value\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(\n",
    "    df2['value'].eqNullSafe(None),\n",
    "    df2['value'].eqNullSafe(float('NaN')),\n",
    "    df2['value'].eqNullSafe(42.0)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1 = df1.toPandas()\n",
    "pdf2 = df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1.join(pdf2, on=\"value\", lsuffix=\"_1\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
