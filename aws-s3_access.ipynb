{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon S3\n",
    "\n",
    "### What it is S3\n",
    "\n",
    "__Amazon S3__ (Simple Storage Service) is a Amazon's service for storing files. It is simple in a sense that one store data using the follwing:\n",
    "* __bucket__: place to store. Its name is unique for all S3 users, which means that there cannot exist two buckets with the same name even if they are private for to different users.\n",
    "* __key__: a unique (for a bucket) name that link to the sotred object. It is common to use path like syntax to group objects. \n",
    "* __object__: any file (text or binary). It can be partitioned.\n",
    "\n",
    "### Sign up\n",
    "First go to \n",
    "<https://s3.console.aws.amazon.com/s3>\n",
    "\n",
    "and sign up for S3. You can also try to create a bucket, upload files etc. Here we will explain how to use it porogramatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing AWS Command Line Interface and boto\n",
    "\n",
    "In order to install boto (Python interface to Amazon Web Service) and AWS Command Line Interface (__CLI__) type:\n",
    "```\n",
    "pip install boto3\n",
    "pip install awscli\n",
    "```\n",
    "\n",
    "Then in your home directory create file `~/.aws/credentials` with the following:\n",
    "\n",
    "```\n",
    "[myaws]\n",
    "aws_access_key_id = YOUR_ACCESS_KEY\n",
    "aws_secret_access_key = YOUR_SECRET_KEY\n",
    "```\n",
    "\n",
    "If you add these configuration as `[default]`, you won't need to add `--profile myaws` in CLI commands in Section CLI Basic Commands.\n",
    "\n",
    "### Where to get credentials from\n",
    "\n",
    "1. Go to https://console.aws.amazon.com/console/home and log in\n",
    "2. Click on USER NAME (right top) and select `My Security Credentials`.\n",
    "3. Click on `+ Access keys (access key ID and secret access key)` and then on `Create New Acess Key`.\n",
    "4 Choose `Show access key`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Basic Commands \n",
    "\n",
    "### List buckets\n",
    "```\n",
    "aws --profile myaws s3 ls\n",
    "```\n",
    "\n",
    "### List all buckets\n",
    "\n",
    "```\n",
    "aws --profile myaws s3 ls \n",
    "```\n",
    "\n",
    "### Create buckers\n",
    "```\n",
    "aws --profile myaws s3 mb s3://julc-public-test\n",
    "```\n",
    "__Warning__ The bucket namespace is shared by all users of the system so you need to change the name.\n",
    "\n",
    "### Upload and download files\n",
    "\n",
    "#### Upload\n",
    "```\n",
    "aws --profile myaws s3 cp data/ReleaseNotes_Spark2-4.txt s3://julc-public-test/ReleaseNotes\n",
    "aws --profile myaws s3 cp data/ReleaseNotes_Spark3.txt s3://julc-public-test/ReleaseNotes\n",
    "```\n",
    "\n",
    "#### Download\n",
    "```\n",
    "aws --profile myaws s3 cp s3://julc-public-test/ReleaseNotes/ReleaseNotes_Spark2-4.txt data/ReleaseNotes_Spark.txt\n",
    "```\n",
    "\n",
    "### List files in path\n",
    " \n",
    "```\n",
    "aws --profile myaws s3 ls s3://julc-public-test\n",
    "aws --profile myaws s3 ls s3://julc-public-test/ReleaseNotes\n",
    "```\n",
    "\n",
    "### Remove file(s)\n",
    "\n",
    "```\n",
    "aws --profile myaws s3 rm s3://julc-public-test/ReleaseNotes/ReleaseNotes_Spark3.txt\n",
    "aws --profile myaws s3 rm s3://julc-public-test/ReleaseNotes/ --recursive\n",
    "```\n",
    "\n",
    "### Delete bucket\n",
    "\n",
    "For deleting a bucket use\n",
    "```\n",
    "aws --profile myaws s3 rb s3://julc-public-test\n",
    "```\n",
    "in order to delete non empty backet use `--force` option.\n",
    "\n",
    "In order to empty a backet use\n",
    "```\n",
    "aws --profile myaws s3 rm s3://julc-public-test --force\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Boto is\n",
    "\n",
    "Boto is a Python package that provides interfaces to Amazon Web Services. Here we are focused on its application to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating S3 Resource\n",
    "\n",
    "We start using boto3 by creating S3 resorce object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "session = boto3.Session(profile_name='myaws')\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From evironment variables\n",
    "\n",
    "If your credentials are stored as evirionment variables `AWS_SECRET_KEY_ID` and `AWS_SECRET_ACCESS_KEY` then you can do the following:\n",
    "\n",
    "```\n",
    "import os\n",
    "aws_access_key_id = os.environ.get('AWS_SECRET_KEY_ID')\n",
    "aws_secret_access_key = s.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id, \n",
    "    aws_secret_access_key=aws_secret_access_key)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.Bucket(name='aws-logs-429368163154-eu-west-3'),\n",
       " s3.Bucket(name='julc-databricks'),\n",
       " s3.Bucket(name='julc-public-test'),\n",
       " s3.Bucket(name='julc-spark')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s3.buckets.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bucket\n",
    "\n",
    "__Warning__ As before, bucket's namespace is shared, so the following command may not poroduce a bucket if a bucket with the name exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3.create_bucket(\n",
    "#    ACL='public-read',\n",
    "#    Bucket=\"julc-public-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you have the followng Access Control List (ACL) options while creating it: \n",
    "* `'private', \n",
    "* 'public-read', \n",
    "* 'public-read-write', \n",
    "* 'authenticated-read'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket = s3.Bucket('julc-public-test')\n",
    "#bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List keys in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.ObjectSummary(bucket_name='julc-public-test', key='LICENSE_spark.txt'),\n",
       " s3.ObjectSummary(bucket_name='julc-public-test', key='ReleaseNotes/'),\n",
       " s3.ObjectSummary(bucket_name='julc-public-test', key='ReleaseNotes/ReleaseNotes_Spark2-4.txt'),\n",
       " s3.ObjectSummary(bucket_name='julc-public-test', key='ReleaseNotes/ReleaseNotes_Spark3.txt'),\n",
       " s3.ObjectSummary(bucket_name='julc-public-test', key='data/daily-total-sales/_SUCCESS'),\n",
       " s3.ObjectSummary(bucket_name='julc-public-test', key='data/daily-total-sales/part-00000-57aff355-5de4-4ffe-94c3-4ae4fe4da2df-c000.snappy.parquet'),\n",
       " s3.ObjectSummary(bucket_name='julc-public-test', key='data/sales_train.csv.gz')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = s3.Bucket('julc-public-test')\n",
    "objs = [obj for obj in bucket.objects.all()]\n",
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ReleaseNotes/ReleaseNotes_Spark2-4.txt',\n",
       " 'ReleaseNotes/ReleaseNotes_Spark3.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj.key for obj in bucket.objects.filter(Prefix=\"ReleaseNotes/\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of class `ObjectSummary` has to properties `Bucket` (that returns Bucket object), `bucket_name` and `key` that return strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(s3.Bucket(name='julc-public-test'), 'julc-public-test', 'LICENSE_spark.txt')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs[0].Bucket(), objs[0].bucket_name, objs[0].key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter keys and sort them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [obj for obj in bucket.objects.filter(Prefix=\"ReleaseNotes/\")]\n",
    "objects.sort(key=lambda obj: obj.key, reverse=True)\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('julc-public-test')\n",
    "bucket.download_file('ReleaseNotes/ReleaseNotes_Spark2-4.txt', \"Spark2-4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_bucket = s3.Bucket(\"julc-public-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_bucket.upload_file(\"data/competitive-data-science-predict-future-sales/sales_train.csv.gz\", 'data/sales_train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(stat_bucket.objects.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = s3.Object('julc-public-test', 'data/Words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links:\n",
    "\n",
    "* https://github.com/boto/boto3\n",
    "* https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "* https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access to s3 from pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dependency\n",
    "Eiter \n",
    "* start pyspark with\n",
    "```pyspark --packages=org.apache.hadoop:hadoop-aws:2.7.3```\n",
    "* add the dependency to SparkSession configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark course\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read aws configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "aws_profile = \"myaws\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", access_id)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", access_key)\n",
    "#hadoop_conf.set(\n",
    "#        'fs.s3a.aws.credentials.provider',\n",
    "#        'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'\n",
    "#    )\n",
    "#hadoop_conf.set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\n",
    "#hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myRDD = sc.textFile(\"s3a://supergloospark/baby_names.csv\")\n",
    "myRDD = sc.textFile(\"s3a://julc-public-test/LICENSE_spark.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                                 Apache License',\n",
       " '                           Version 2.0, January 2004',\n",
       " '                        http://www.apache.org/licenses/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.option(\"header\", \"true\").csv(\"s3a://julc-public-test/data/sales_train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------+-------+----------+------------+\n",
      "|      date|date_block_num|shop_id|item_id|item_price|item_cnt_day|\n",
      "+----------+--------------+-------+-------+----------+------------+\n",
      "|02.01.2013|             0|     59|  22154|     999.0|         1.0|\n",
      "|03.01.2013|             0|     25|   2552|     899.0|         1.0|\n",
      "|05.01.2013|             0|     25|   2552|     899.0|        -1.0|\n",
      "|06.01.2013|             0|     25|   2554|   1709.05|         1.0|\n",
      "|15.01.2013|             0|     25|   2555|    1099.0|         1.0|\n",
      "|10.01.2013|             0|     25|   2564|     349.0|         1.0|\n",
      "|02.01.2013|             0|     25|   2565|     549.0|         1.0|\n",
      "|04.01.2013|             0|     25|   2572|     239.0|         1.0|\n",
      "|11.01.2013|             0|     25|   2572|     299.0|         1.0|\n",
      "|03.01.2013|             0|     25|   2573|     299.0|         3.0|\n",
      "|03.01.2013|             0|     25|   2574|     399.0|         2.0|\n",
      "|05.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "|07.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "|08.01.2013|             0|     25|   2574|     399.0|         2.0|\n",
      "|10.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "|11.01.2013|             0|     25|   2574|     399.0|         2.0|\n",
      "|13.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "|16.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "|26.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "|27.01.2013|             0|     25|   2574|     399.0|         1.0|\n",
      "+----------+--------------+-------+-------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "sdf.groupBy(\"date\").agg(F.sum(F.col('item_cnt_day')).alias(\"items\"))\\\n",
    "    .repartition(1)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .parquet(\"s3a://julc-public-test/data/daily-total-sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"s3a://julc-public-test/data/daily-total-sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
